diff --git a/src/file_duplicate_scanner.py b/src/file_duplicate_scanner.py
index b638ad7..0a62398 100644
--- a/src/file_duplicate_scanner.py
+++ b/src/file_duplicate_scanner.py
@@ -42,6 +42,8 @@ class ScanStats:
     dirs_skipped: int = 0
     hidden_files_skipped: int = 0
     removed_count: int = 0
+    total_size_scanned: int = 0
+    potential_savings: int = 0


 def create_database(db_path: str) -> sqlite3.Connection:
@@ -55,10 +57,18 @@ def create_database(db_path: str) -> sqlite3.Connection:
             file_path TEXT NOT NULL UNIQUE,
             sha1_hash TEXT NOT NULL,
             modified_time TIMESTAMP NOT NULL,
+            file_size INTEGER NOT NULL DEFAULT 0,
             scan_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
         )
     ''')

+    # Handle migration for existing databases - add file_size column if it doesn't exist
+    cursor.execute("PRAGMA table_info(files)")
+    columns = [column[1] for column in cursor.fetchall()]
+    if 'file_size' not in columns:
+        cursor.execute('ALTER TABLE files ADD COLUMN file_size INTEGER NOT NULL DEFAULT 0')
+        print("Added file_size column to existing database.")
+
     # Create table for ignored duplicate sets
     cursor.execute('''
         CREATE TABLE IF NOT EXISTS ignored_duplicates (
@@ -175,13 +185,13 @@ def should_skip_file(file_path: str, include_hidden: bool) -> bool:
 def get_existing_files_info(conn: sqlite3.Connection) -> dict:
     """
     Get information about files already in the database.
-    Returns dict: {absolute_file_path: modified_time}
+    Returns dict: {absolute_file_path: (modified_time, file_size)}
     """
     cursor = conn.cursor()
-    cursor.execute('SELECT file_path, modified_time FROM files')
+    cursor.execute('SELECT file_path, modified_time, file_size FROM files')

     existing_files = {}
-    for file_path, modified_time_str in cursor.fetchall():
+    for file_path, modified_time_str, file_size in cursor.fetchall():
         # Convert string back to datetime for comparison
         try:
             modified_time = datetime.fromisoformat(modified_time_str.replace('Z', '+00:00')) if 'Z' in modified_time_str else datetime.fromisoformat(modified_time_str)
@@ -192,7 +202,7 @@ def get_existing_files_info(conn: sqlite3.Connection) -> dict:
             except ValueError:
                 modified_time = datetime.strptime(modified_time_str, '%Y-%m-%d %H:%M:%S')

-        existing_files[file_path] = modified_time
+        existing_files[file_path] = (modified_time, file_size or 0)

     return existing_files

@@ -296,7 +306,7 @@ def check_file_needs_processing(abs_file_path: str, existing_files: dict, curren

     Args:
         abs_file_path: Absolute path to the file
-        existing_files: Dictionary of existing files and their modification times
+        existing_files: Dictionary of existing files with (modified_time, file_size) tuples
         current_modified_time: Current modification time of the file

     Returns:
@@ -309,7 +319,7 @@ def check_file_needs_processing(abs_file_path: str, existing_files: dict, curren
         return True, True
     else:
         # Compare modification times
-        db_modified_time = existing_files[abs_file_path]
+        db_modified_time, _ = existing_files[abs_file_path]  # Extract modified_time from tuple
         # Allow for small time differences due to precision
         time_diff = abs((current_modified_time - db_modified_time).total_seconds())
         if time_diff > 1:  # More than 1 second difference
@@ -320,7 +330,7 @@ def check_file_needs_processing(abs_file_path: str, existing_files: dict, curren


 def process_file_to_database(abs_file_path: str, sha1_hash: str, current_modified_time: datetime,
-                           is_new_file: bool, cursor: sqlite3.Cursor) -> None:
+                           file_size: int, is_new_file: bool, cursor: sqlite3.Cursor) -> None:
     """
     Insert or update file information in the database.

@@ -328,22 +338,23 @@ def process_file_to_database(abs_file_path: str, sha1_hash: str, current_modifie
         abs_file_path: Absolute path to the file
         sha1_hash: SHA1 hash of the file
         current_modified_time: Current modification time
+        file_size: Size of the file in bytes
         is_new_file: Whether this is a new file or an update
         cursor: Database cursor
     """
     if is_new_file:
         # Insert new file with absolute path
         cursor.execute('''
-            INSERT INTO files (file_path, sha1_hash, modified_time)
-            VALUES (?, ?, ?)
-        ''', (abs_file_path, sha1_hash, current_modified_time))
+            INSERT INTO files (file_path, sha1_hash, modified_time, file_size)
+            VALUES (?, ?, ?, ?)
+        ''', (abs_file_path, sha1_hash, current_modified_time, file_size))
     else:
         # Update existing file
         cursor.execute('''
             UPDATE files
-            SET sha1_hash = ?, modified_time = ?, scan_time = CURRENT_TIMESTAMP
+            SET sha1_hash = ?, modified_time = ?, file_size = ?, scan_time = CURRENT_TIMESTAMP
             WHERE file_path = ?
-        ''', (sha1_hash, current_modified_time, abs_file_path))
+        ''', (sha1_hash, current_modified_time, file_size, abs_file_path))


 def process_single_file(file_path: str, root: str, include_hidden: bool, existing_files: dict,
@@ -403,8 +414,15 @@ def process_single_file(file_path: str, root: str, include_hidden: bool, existin
         if not sha1_hash:  # Skip if hash calculation failed
             return

+        # Get file size
+        try:
+            file_size = os.path.getsize(abs_file_path)
+        except (IOError, OSError) as e:
+            print(f"Error getting file size for {abs_file_path}: {e}")
+            return
+
         # Process to database
-        process_file_to_database(abs_file_path, sha1_hash, current_modified_time, is_new_file, cursor)
+        process_file_to_database(abs_file_path, sha1_hash, current_modified_time, file_size, is_new_file, cursor)

         if is_new_file:
             stats.files_added += 1
@@ -412,6 +430,7 @@ def process_single_file(file_path: str, root: str, include_hidden: bool, existin
             stats.files_updated += 1

         stats.files_processed += 1
+        stats.total_size_scanned += file_size

         # Commit periodically for large scans
         if stats.files_processed % 100 == 0:
@@ -432,6 +451,9 @@ def print_scan_results(stats: ScanStats) -> None:
     print(f"  Hidden files skipped: {stats.hidden_files_skipped}")
     print(f"  Directories skipped: {stats.dirs_skipped}")
     print(f"  Total files processed: {stats.files_processed}")
+    print(f"  Total size scanned: {format_file_size(stats.total_size_scanned)}")
+    if stats.potential_savings > 0:
+        print(f"  Potential savings found: {format_file_size(stats.potential_savings)}")


 def scan_files(directory_path: str, conn: sqlite3.Connection, include_hidden: bool = False, excluded_dirs: List[str] = None, stop_event=None) -> int:
@@ -511,7 +533,7 @@ def scan_files(directory_path: str, conn: sqlite3.Connection, include_hidden: bo
     return stats.files_processed


-def find_duplicates(conn: sqlite3.Connection, directory_filter: str = None, include_ignored: bool = False) -> List[Tuple[str, List[str]]]:
+def find_duplicates(conn: sqlite3.Connection, directory_filter: str = None, include_ignored: bool = False) -> List[Tuple[str, List[str], int, int]]:
     """
     Find duplicate files based on SHA1 hash.

@@ -521,7 +543,8 @@ def find_duplicates(conn: sqlite3.Connection, directory_filter: str = None, incl
         include_ignored: Whether to include ignored duplicate sets

     Returns:
-        List of tuples: (hash, [list of file paths with that hash])
+        List of tuples: (hash, [list of file paths with that hash], file_size, potential_savings)
+        where potential_savings = (duplicate_count - 1) * file_size
     """
     cursor = conn.cursor()

@@ -561,9 +584,9 @@ def find_duplicates(conn: sqlite3.Connection, directory_filter: str = None, incl
     duplicates = []

     for sha1_hash, count in duplicate_hashes:
-        # Get all file paths for this hash
+        # Get all file paths and file size for this hash
         file_query = '''
-            SELECT file_path, modified_time
+            SELECT file_path, modified_time, file_size
             FROM files
             WHERE sha1_hash = ?
         '''
@@ -578,11 +601,14 @@ def find_duplicates(conn: sqlite3.Connection, directory_filter: str = None, incl
         file_query += " ORDER BY modified_time"

         cursor.execute(file_query, file_params)
-        file_paths = [row[0] for row in cursor.fetchall()]
+        results = cursor.fetchall()
+        file_paths = [row[0] for row in results]

         # Only include if we still have duplicates after filtering
         if len(file_paths) > 1:
-            duplicates.append((sha1_hash, file_paths))
+            file_size = results[0][2] if results else 0  # All duplicates have the same size
+            potential_savings = (len(file_paths) - 1) * file_size
+            duplicates.append((sha1_hash, file_paths, file_size, potential_savings))

     return duplicates

@@ -633,33 +659,56 @@ def unignore_duplicate_set(conn: sqlite3.Connection, sha1_hash: str) -> bool:
         return False


-def print_duplicates(duplicates: List[Tuple[str, List[str]]]) -> None:
-    """Print duplicate files in a readable format."""
+def print_duplicates(duplicates: List[Tuple[str, List[str], int, int]]) -> None:
+    """Print duplicate files in a readable format with potential savings."""
     if not duplicates:
         print("\nNo duplicate files found.")
         return

+    # Calculate total potential savings
+    total_savings = sum(potential_savings for _, _, _, potential_savings in duplicates)
+
     print(f"\nFound {len(duplicates)} sets of duplicate files:")
+    print(f"Total potential storage savings: {format_file_size(total_savings)}")
     print("=" * 60)

-    for i, (sha1_hash, file_paths) in enumerate(duplicates, 1):
+    for i, (sha1_hash, file_paths, file_size, potential_savings) in enumerate(duplicates, 1):
         print(f"\nDuplicate Set #{i} (SHA1: {sha1_hash[:16]}...):")
-        print(f"  {len(file_paths)} identical files:")
+        print(f"  {len(file_paths)} identical files, {format_file_size(file_size)} each")
+        print(f"  Potential savings: {format_file_size(potential_savings)}")
         for file_path in file_paths:
             print(f"    - {file_path}")


+def format_file_size(size_bytes: int) -> str:
+    """Format file size in human-readable format."""
+    if size_bytes == 0:
+        return "0 B"
+    size_names = ["B", "KB", "MB", "GB", "TB"]
+    import math
+    i = int(math.floor(math.log(size_bytes, 1024)))
+    p = math.pow(1024, i)
+    s = round(size_bytes / p, 2)
+    return f"{s} {size_names[i]}"
+
+
 def print_database_stats(conn: sqlite3.Connection) -> None:
     """Print statistics about the database."""
     cursor = conn.cursor()

-    # Total files
-    cursor.execute('SELECT COUNT(*) FROM files')
-    total_files = cursor.fetchone()[0]
+    # Total files and total size
+    cursor.execute('SELECT COUNT(*), COALESCE(SUM(file_size), 0) FROM files')
+    total_files, total_size = cursor.fetchone()

-    # Unique hashes
-    cursor.execute('SELECT COUNT(DISTINCT sha1_hash) FROM files')
-    unique_hashes = cursor.fetchone()[0]
+    # Unique files and unique size (sum of sizes for unique hashes)
+    cursor.execute('''
+        SELECT COUNT(DISTINCT sha1_hash), COALESCE(SUM(min_size), 0) FROM (
+            SELECT sha1_hash, MIN(file_size) as min_size
+            FROM files
+            GROUP BY sha1_hash
+        )
+    ''')
+    unique_hashes, unique_size = cursor.fetchone()

     # Files with duplicates
     cursor.execute('''
@@ -672,16 +721,29 @@ def print_database_stats(conn: sqlite3.Connection) -> None:
     ''')
     duplicate_files = cursor.fetchone()[0]

+    # Calculate potential savings
+    duplicates = find_duplicates(conn, include_ignored=False)
+    potential_savings = sum(potential_savings for _, _, _, potential_savings in duplicates)
+
     # Ignored duplicate sets
     cursor.execute('SELECT COUNT(*) FROM ignored_duplicates')
     ignored_sets = cursor.fetchone()[0]

     print(f"\nDatabase Statistics:")
     print(f"  Total files: {total_files}")
+    print(f"  Total size: {format_file_size(total_size)}")
     print(f"  Unique files: {unique_hashes}")
+    print(f"  Unique size: {format_file_size(unique_size)}")
     print(f"  Duplicate files: {duplicate_files}")
+    print(f"  Potential storage savings: {format_file_size(potential_savings)}")
     print(f"  Ignored duplicate sets: {ignored_sets}")
-    print(f"  Storage efficiency: {((unique_hashes/total_files)*100):.1f}%" if total_files > 0 else "  Storage efficiency: N/A")
+
+    # Storage efficiency based on file size rather than file count
+    if total_size > 0:
+        storage_efficiency = (unique_size / total_size) * 100
+        print(f"  Storage efficiency: {storage_efficiency:.1f}% (based on file size)")
+    else:
+        print(f"  Storage efficiency: N/A")


 def main():
@@ -775,7 +837,7 @@ Examples:

         # Return list of all duplicate file paths
         all_duplicate_paths = []
-        for _, file_paths in duplicates:
+        for _, file_paths, _, _ in duplicates:
             all_duplicate_paths.extend(file_paths)

         if args.verbose and all_duplicate_paths:
diff --git a/src/web_scanner_gui.py b/src/web_scanner_gui.py
index 6cb2ffb..dd3e7d3 100644
--- a/src/web_scanner_gui.py
+++ b/src/web_scanner_gui.py
@@ -130,13 +130,19 @@ def get_stats():
         conn = create_database(db_path)
         cursor = conn.cursor()

-        # Total files
-        cursor.execute('SELECT COUNT(*) FROM files')
-        total_files = cursor.fetchone()[0]
+        # Total files and total size
+        cursor.execute('SELECT COUNT(*), COALESCE(SUM(file_size), 0) FROM files')
+        total_files, total_size = cursor.fetchone()

-        # Unique hashes
-        cursor.execute('SELECT COUNT(DISTINCT sha1_hash) FROM files')
-        unique_hashes = cursor.fetchone()[0]
+        # Unique files and unique size (sum of sizes for unique hashes)
+        cursor.execute('''
+            SELECT COUNT(DISTINCT sha1_hash), COALESCE(SUM(min_size), 0) FROM (
+                SELECT sha1_hash, MIN(file_size) as min_size
+                FROM files
+                GROUP BY sha1_hash
+            )
+        ''')
+        unique_hashes, unique_size = cursor.fetchone()

         # Files with duplicates
         cursor.execute('''
@@ -149,6 +155,10 @@ def get_stats():
         ''')
         duplicate_files = cursor.fetchone()[0]

+        # Calculate potential savings
+        duplicates = find_duplicates(conn, include_ignored=False)
+        potential_savings = sum(potential_savings for _, _, _, potential_savings in duplicates)
+
         # Ignored duplicate sets
         cursor.execute('SELECT COUNT(*) FROM ignored_duplicates')
         ignored_sets = cursor.fetchone()[0]
@@ -170,14 +180,18 @@ def get_stats():

         conn.close()

-        storage_efficiency = ((unique_hashes/total_files)*100) if total_files > 0 else 0
+        # Storage efficiency based on file size rather than file count
+        storage_efficiency = ((unique_size / total_size) * 100) if total_size > 0 else 0

         return jsonify({
             'success': True,
             'stats': {
                 'total_files': total_files,
+                'total_size': total_size,
                 'unique_files': unique_hashes,
+                'unique_size': unique_size,
                 'duplicate_files': duplicate_files,
+                'potential_savings': potential_savings,
                 'ignored_sets': ignored_sets,
                 'storage_efficiency': round(storage_efficiency, 1),
                 'database_size_mb': round(db_size_mb, 2),
@@ -207,12 +221,14 @@ def load_duplicates():

         # Format duplicates for web display
         formatted_duplicates = []
-        for i, (hash_value, file_paths) in enumerate(duplicates):
+        for i, (hash_value, file_paths, file_size, potential_savings) in enumerate(duplicates):
             duplicate_set = {
                 'id': i,
                 'hash': hash_value,
                 'hash_short': hash_value[:16] + "...",
                 'files': [],
+                'file_size': file_size,
+                'potential_savings': potential_savings,
                 'ignored': False
             }

diff --git a/templates/index.html b/templates/index.html
index f97f74e..84f7f6d 100644
--- a/templates/index.html
+++ b/templates/index.html
@@ -1085,6 +1085,18 @@
                                 <div class="stat-value" id="stat-db-size">-</div>
                                 <div class="stat-label">Database Size</div>
                             </div>
+                            <div class="stat-card">
+                                <div class="stat-value" id="stat-total-size">-</div>
+                                <div class="stat-label">Total Size</div>
+                            </div>
+                            <div class="stat-card">
+                                <div class="stat-value" id="stat-unique-size">-</div>
+                                <div class="stat-label">Unique Size</div>
+                            </div>
+                            <div class="stat-card">
+                                <div class="stat-value" id="stat-potential-savings">-</div>
+                                <div class="stat-label">Potential Savings</div>
+                            </div>
                         </div>

                         <button class="btn btn-secondary" onclick="loadStats()">ðŸ”„ Refresh Statistics</button>
@@ -1110,6 +1122,15 @@
             'stats': 'ðŸ“ˆ Performance Analytics Dashboard'
         };

+        function formatFileSize(sizeBytes) {
+            if (sizeBytes === 0) return '0 B';
+            const sizeNames = ['B', 'KB', 'MB', 'GB', 'TB'];
+            const i = Math.floor(Math.log(sizeBytes) / Math.log(1024));
+            const p = Math.pow(1024, i);
+            const s = Math.round((sizeBytes / p) * 100) / 100;
+            return s + ' ' + sizeNames[i];
+        }
+
         function showToast(message, type = 'success') {
             const toastContainer = document.getElementById('toast-container');
             const toast = document.createElement('div');
@@ -1288,6 +1309,9 @@
                     document.getElementById('stat-ignored-sets').textContent = stats.ignored_sets.toLocaleString();
                     document.getElementById('stat-efficiency').textContent = stats.storage_efficiency + '%';
                     document.getElementById('stat-db-size').textContent = stats.database_size_mb + ' MB';
+                    document.getElementById('stat-total-size').textContent = formatFileSize(stats.total_size || 0);
+                    document.getElementById('stat-unique-size').textContent = formatFileSize(stats.unique_size || 0);
+                    document.getElementById('stat-potential-savings').textContent = formatFileSize(stats.potential_savings || 0);
                 } else {
                     showToast('Error loading statistics: ' + result.error, 'error');
                 }
@@ -1439,7 +1463,8 @@
                         <div style="display: flex; align-items: center; gap: 12px;">
                             <div>
                                 <strong>Duplicate Set #${setIndex + 1}</strong>
-                                (${dupSet.files.length} files) - Hash: ${dupSet.hash_short}
+                                (${dupSet.files.length} files, ${formatFileSize(dupSet.file_size || 0)} each) - Hash: ${dupSet.hash_short}
+                                <br><small style="color: var(--warning-color);">ðŸ’¾ Potential savings: ${formatFileSize(dupSet.potential_savings || 0)}</small>
                             </div>
                             ${dupSet.ignored ? '<span class="status-badge status-warning">IGNORED</span>' : ''}
                         </div>
